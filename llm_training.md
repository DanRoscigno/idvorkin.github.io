---
layout: post
title: "Training LLMs"
permalink: /llm-training
redirect-from:
  - /ai-training
---

Generally, I don't deal with training or deeper tech, but might as well keep some onotes on it.

<!-- prettier-ignore-start -->
<!-- vim-markdown-toc GFM -->

- [Training](#training)
    - [RAG Grounding](#rag-grounding)
    - [Fine Tuning](#fine-tuning)
    - [Style transfer](#style-transfer)
    - [Model Merging](#model-merging)
    - [Model Merging](#model-merging-1)
    - [Awesome blog](#awesome-blog)
- [Quantization](#quantization)

<!-- vim-markdown-toc -->
<!-- prettier-ignore-end -->

## Training

### RAG Grounding

### Fine Tuning

### Style transfer

### Model Merging

### Model Merging

### Awesome blog

[Fine-Tune Your Own Llama 2 Model in a Colab Notebook
](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html)

## Quantization

Not quite training but need to stuff this somewhere...

https://mlabonne.github.io/blog/posts/Introduction_to_Weight_Quantization.html
